{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import glob \n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from osgeo import gdal, osr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from shapely.geometry import LinearRing, Point, Polygon, box\n",
    "import rasterio\n",
    "import requests\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdal_load(path, band):\n",
    "    \"\"\"\n",
    "    Load image file into array\n",
    "    using gdal.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    path = path to image file, text.\n",
    "    band = band to load, int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loaded file as numpy array\n",
    "    \"\"\"\n",
    "    ds = gdal.Open(path, gdal.GA_ReadOnly)\n",
    "    arr = ds.GetRasterBand(band).ReadAsArray()\n",
    "    #length, width = ds.RasterYSize, ds.RasterXSize\n",
    "    # We also save the raster geo info to use them for plotting later \n",
    "    # ulx, uly stands for upper left corner, lrx, lry for lower right corner\n",
    "    ulx, xres, xskew, uly, yskew, yres  = ds.GetGeoTransform()\n",
    "    x = ulx + np.arange(ds.RasterXSize, dtype=float) * xres\n",
    "    y = uly + np.arange(ds.RasterYSize, dtype=float) * yres\n",
    "    # this empties the gdal raster variable that we no longer need since the raster has been saved into and array\n",
    "    ds = None \n",
    "    \n",
    "    return arr, x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime(filename):\n",
    "    \"\"\"\n",
    "    Extract a datetime string in the format '_YYYYMMDDTHHMMSSZ' from the filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Filename containing the date/time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datetime.datetime or None\n",
    "        Parsed datetime object or None if not found.\n",
    "    \"\"\"\n",
    "    #match = re.search(r'(\\d{8}T\\d{6}Z)', filename)  # Get start date\n",
    "    match = re.search(r'_(\\d{8}T\\d{6}Z)', filename)  # Get end date\n",
    "    if match:\n",
    "        #return datetime.strptime(match.group(), \"%Y%m%dT%H%M%SZ\")  # Start date to datetime object\n",
    "        return datetime.strptime(match.group(1), \"%Y%m%dT%H%M%SZ\")  # End date to datetime object\n",
    "    return None\n",
    "\n",
    "def load_data_stack(download_folder, data_type, start_datetime, end_datetime):\n",
    "    \"\"\"\n",
    "    Load and stack WTR data for all files in a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    download_folder : str\n",
    "        Folder path containing WTR GeoTIFF files.\n",
    "    data_type : str\n",
    "        'HLS' or 'S1' type used for file filtering.\n",
    "    start_datetime : datetime\n",
    "        Start of time range.\n",
    "    end_datetime : datetime\n",
    "        End of time range.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_WTR : np.ndarray\n",
    "        Stacked 3D array (rows x cols x time).\n",
    "    \"\"\"\n",
    "    # Find matching WTR files\n",
    "    files_WTR = sorted(glob.glob(download_folder + f\"/*-{data_type}_*_WTR.tif\"))\n",
    "    print(f\"# {data_type} WTR: {len(files_WTR)}\")\n",
    "    \n",
    "    data_stack = []\n",
    "\n",
    "    for fp in files_WTR:\n",
    "        date_fp = extract_datetime(fp)  # Extract timestamp from filename\n",
    "        if date_fp < start_datetime:\n",
    "            continue  # Skip files outside the date range\n",
    "        if date_fp > end_datetime:\n",
    "            continue  # Skip files outside the date range\n",
    "\n",
    "        data, x, y = gdal_load(fp, 1)\n",
    "        data_stack.append(data)\n",
    "\n",
    "    data_WTR = np.stack(data_stack, axis = -1) # stack of all historical HLS data\n",
    "\n",
    "    return data_WTR, x, y\n",
    "\n",
    "def generate_occurrence_map(data, target_value=1):\n",
    "    \"\"\"\n",
    "    Generate an occurrence probability map for a specific class value (default is water=1),\n",
    "    excluding cloud (253) and no-data (255) pixels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        A 3D numpy array (height x width x time).\n",
    "    target_value : int, optional\n",
    "        The value whose occurrence probability is to be computed (default is 1 for open water).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    occurrence_map : numpy.ndarray\n",
    "        A 2D array with occurrence probabilities (float in [0,1]).\n",
    "    \"\"\"\n",
    "    # Define valid pixels: exclude cloud (253) and nonvalid (255)\n",
    "    valid_mask = (data != 253) & (data != 255)\n",
    "\n",
    "    # Count how many times target_value appears in valid observations\n",
    "    count_target = np.sum((data == target_value) & valid_mask, axis=-1)\n",
    "\n",
    "    # Count total number of valid observations\n",
    "    count_valid = np.sum(valid_mask, axis=-1)\n",
    "\n",
    "    # Compute probability while avoiding divide-by-zero\n",
    "    occurrence_map = np.divide(\n",
    "        count_target, count_valid,\n",
    "        where=(count_valid > 0),\n",
    "        out=np.full(count_target.shape, np.nan, dtype=float)\n",
    "    )\n",
    "\n",
    "    return occurrence_map\n",
    "\n",
    "def generate_water_occurrence_map(data):\n",
    "    \"\"\"\n",
    "    Generate a water occurrence map for the 3D array excluding invalid data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        3D array (height x width x time) of WTR labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    water_occurrence_map : np.ndarray\n",
    "        2D array showing fraction of valid observations that were water.\n",
    "    \"\"\"\n",
    "    # Mask to exclude cloud(253) and nonvalid(255) pixels\n",
    "    valid_mask = (data != 253) & (data != 255)\n",
    "\n",
    "    # Count the occurrences of open water(1) and valid (non-255) observations for each pixel\n",
    "    count_ones = np.sum((data == 1) & valid_mask, axis=-1)\n",
    "    \n",
    "    # Count number of valid observations\n",
    "    count_valid = np.sum(valid_mask, axis=-1)\n",
    "\n",
    "    # Compute ratio of water presence, avoiding division by zero\n",
    "    water_occurrence_map = np.divide(\n",
    "        count_ones, count_valid, where=(count_valid > 0), \n",
    "        out=np.full(count_ones.shape, np.nan, dtype=float)\n",
    "    )\n",
    "\n",
    "    return water_occurrence_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dswx(polys, \n",
    "                      datetime_start_str, \n",
    "                      datetime_end_str, \n",
    "                      MGRS_tile,\n",
    "                      data_type,\n",
    "                      download_dir,\n",
    "                      download_flag=True):\n",
    "    \"\"\"\n",
    "    Query and download DSWx files from NASA CMR, then create a water occurrence map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polys : shapely Polygon or box\n",
    "        Area of interest for bounding box.\n",
    "    datetime_start_str : str\n",
    "        ISO string of start datetime.\n",
    "    datetime_end_str : str\n",
    "        ISO string of end datetime.\n",
    "    MGRS_tile : str\n",
    "        Tile ID to filter files.\n",
    "    data_type : str\n",
    "        Either 'HLS' or 'S1'.\n",
    "    download_dir : str\n",
    "        Folder to store downloaded data.\n",
    "    download_flag : bool\n",
    "        Whether to download files or just simulate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    water_occurrence_map : np.ndarray\n",
    "        Resulting water probability map.\n",
    "    \"\"\"\n",
    "    CMR_OPS = 'https://cmr.earthdata.nasa.gov/search'  # NASA's Common Metadata Repository endpoint\n",
    "    url = f'{CMR_OPS}/{\"granules\"}'  # Search endpoint\n",
    "    boundind_box = polys.bounds\n",
    "    provider = 'POCLOUD'\n",
    "\n",
    "    # Parse date strings into datetime objects\n",
    "    start_datetime = datetime.strptime(datetime_start_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_datetime = datetime.strptime(datetime_end_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # Set collection ID for desired dataset\n",
    "    if data_type == 'HLS':\n",
    "        concept_id = 'C2617126679-POCLOUD'\n",
    "    if data_type == 'S1':\n",
    "        concept_id = 'C2949811996-POCLOUD'\n",
    "\n",
    "    # Prepare output folder\n",
    "    download_folder = download_dir + 'download_' + mgrs_tile + '_water_occurrence'\n",
    "    os.makedirs(download_folder, exist_ok = True)\n",
    "\n",
    "    # iterate for 30 days intervals\n",
    "    current_start = start_datetime\n",
    "    while current_start < end_datetime:\n",
    "        # Calculate the end of the current month interval\n",
    "        current_end = min (current_start + timedelta(days = 30), end_datetime)\n",
    "        print(current_start)\n",
    "        print(current_end)\n",
    "\n",
    "        parameters = {\n",
    "            'temporal': f'{current_start.strftime(\"%Y-%m-%dT%H:%M:%SZ\")},{current_end.strftime(\"%Y-%m-%dT%H:%M:%SZ\")}',\n",
    "            'concept_id': concept_id,\n",
    "            'provider': provider,\n",
    "            'bounding_box': f'{boundind_box[1]},{boundind_box[0]},{boundind_box[3]},{boundind_box[2]}',\n",
    "            'page_size': 2000,\n",
    "            }\n",
    "        print(parameters)\n",
    "        response = requests.get(url,\n",
    "            params=parameters,\n",
    "            headers={'Accept': 'application/json'}\n",
    "            )\n",
    "        print(response.status_code)\n",
    "        print(response.headers['CMR-Hits'])\n",
    "\n",
    "        downloaded_list = []\n",
    "        num_search_data = response.headers['CMR-Hits']\n",
    "        number_data = 0\n",
    "        if num_search_data:\n",
    "            collections = response.json()['feed']['entry']\n",
    "            for collection in collections:\n",
    "                dswx_hls_file_id = collection['producer_granule_id']\n",
    "                if MGRS_tile not in dswx_hls_file_id:\n",
    "                    print('MGRS tile id does not match')\n",
    "                    continue\n",
    "                    \n",
    "                print('url', f'{collection[\"links\"][4][\"href\"]}')\n",
    "                print('s3', f'{collection[\"links\"][3][\"href\"]}')\n",
    "                if download_flag:\n",
    "                    for layer in range(12):#[0, 2, 4, 6, 8, 10]:  # 2: WTR, 4: BWTR, 6:CONF (not always)\n",
    "                        dswx_hls_url = collection[\"links\"][layer][\"href\"]\n",
    "                        dswx_hls_filename = os.path.basename(dswx_hls_url)\n",
    "\n",
    "                        # Filter to only specific layers (STR, BWTR, CONF)\n",
    "                        if not any(substring in dswx_hls_filename for substring in ['_B01_WTR.tif', '_B02_BWTR.tif', '_B03_CONF.tif']): # B01: WTR, B02: BWTR, B03: CONF\n",
    "                            continue\n",
    "                        if any(substring in dswx_hls_filename for substring in ['.md5']):\n",
    "                            continue\n",
    "                            \n",
    "                        download_file = f'{download_folder}/{dswx_hls_filename}'\n",
    "\n",
    "                        if not os.path.isfile(download_file):\n",
    "                            response = requests.get(dswx_hls_url, stream=True)\n",
    "\n",
    "                        # Check if the request was successful\n",
    "                            if response.status_code == 200:\n",
    "                                # Open a local file with wb (write binary) permission.\n",
    "                                with open(f'{download_file}', 'wb') as file:\n",
    "                                    print('downloading')\n",
    "                                    for chunk in response.iter_content(chunk_size=128):\n",
    "                                        file.write(chunk)\n",
    "                else:\n",
    "                    print('under dev.')\n",
    "                downloaded_list.append(download_file)\n",
    "                number_data += 1\n",
    "        \n",
    "        current_start = current_end  # Move to next interval\n",
    "\n",
    "    # Load raster stack and compute water probability\n",
    "    data_stack, x, y = load_data_stack(download_folder, data_type, start_datetime, end_datetime)\n",
    "    water_occurrence_map = generate_occurrence_map(data_stack)\n",
    "\n",
    "    return water_occurrence_map, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== EXAMPLE USAGE ==========\n",
    "# Define bounding box polygon for region of interest\n",
    "poly_cand = box(36.9,-124.4,  39.9,-120.6) #10S\n",
    "#poly_cand = box(33.38,-119.26,  35.3,-115.83) #11S\n",
    "\n",
    "# Date range for querying granules\n",
    "datetime_start_str = '2024-09-01T00:00:00Z'\n",
    "datetime_end_str = '2024-12-31T00:00:00Z'\n",
    "\n",
    "# Dataset type and tile ID\n",
    "data_type = 'HLS'\n",
    "#mgrs_tile = '11SLU'\n",
    "mgrs_tile = '10SFH'\n",
    "\n",
    "# Local directory to save downloaded files\n",
    "download_dir = '/u/aurora-r0/jeon/tools/DSWx-fusion/DSWx-Enhanced-Analysis/'\n",
    "\n",
    "# Run full download and analysis pipeline\n",
    "water_occurrence_map, x, y = download_dswx(poly_cand, datetime_start_str, datetime_end_str, mgrs_tile, data_type, download_dir)  \n",
    "\n",
    "# Plot the resulting water occurrence probability map\n",
    "im = plt.figure()\n",
    "im.set_dpi(300)\n",
    "im.set_size_inches(8,6)\n",
    "plt.imshow(water_occurrence_map, cmap='terrain_r', extent=[x[0], x[-1], y[0], y[-1]]) \n",
    "plt.xlabel('x (m)')\n",
    "plt.ylabel('y (m)')\n",
    "plt.title('Water occurrence map')\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dswx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
